\chapter{Preliminaries}\label{chap:preliminaries}

\section{SAT basics}

\subsection{Satisfiability problem}
A \emph{Boolean variable}, or \emph{propositional variable}, is a variable that
has two possible values : true or false (noted respectively $\true$ or $\false$).
A \emph{literal} $l$ is a propositional variable or its
negation. For a given variable $x$, the positive literal is represented by $x$
and the negative one by $\neg x$.
A \emph{clause} $\omega$ is a finite disjunction of literals represented
equivalently by $\omega = \bigvee_{i=1}^k l_i$ or the set of its literals
$\omega = \{l_i\}_{i \in \llbracket 1,k \rrbracket}$. A clause with a single
literal is called \emph{unit clause}.
A \emph{conjunctive normal form (CNF) formula} $\varphi$ is a finite
conjunction of clauses.  A CNF can be either noted $\varphi = \bigwedge_{i=1}^k
\omega_i$ or $\varphi = \{\omega_i\}_{i \in \llbracket 1,k \rrbracket}$. We
denote $\Vars_\varphi$ ($\Lits_\varphi$) the set of variables (literals) used in
$\varphi$ (the index in $\Vars_\varphi$ and $\Lits_\varphi$ is usually omitted when
clear from context).

For a given formula $\varphi$, an \emph{assignment} of the variables of
$\varphi$ is a function $\alpha: \Vars \mapsto \{ \true, \false \}$.  As usual, $\alpha$ is
\emph{total}, or \emph{complete}, when all elements of $\Vars$ have an image by
$\alpha$, otherwise it is \emph{partial}. By abuse of notation, an assignment is
often represented by the set of its true literals.  The set of all (possibly
partial) assignments of $\Vars$ is noted $\Assignments(\Vars)$.

The assignment $\alpha$ \emph{satisfies} the clause $\omega$, denoted $\alpha
\models \omega$, if $\alpha \cap \omega \neq \emptyset$. Similarly, the assignment
$\alpha$ satisfies the propositional formula $\varphi$, denoted $\alpha \models
\varphi$, if $\alpha$ satisfies all the clauses of $\varphi$. Note that a
formula may be satisfied by a partial assignment. In this case, unassigned variable are called
\emph{dont care}.
A formula is said to be
\emph{satisfiable} (\sat) if there is at least one assignment that satisfies it;
otherwise the formula is \emph{unsatisfiable} (\unsat).

\subsection{An NP-complete problem}

The SAT problems is the first NP-complete algorithm as proven by Stephen Cook in 1971~\cite{cook1971complexity}.
It is also proved by Leonid Levin in 1973~\cite{4640789}, for this purpose, it was known as Cook-Levin theorem.
The proof of to show that can be found in \cite{sipser2006introduction}.
Any NP problem can be reduced to a SAT problem in polynomial time and so open one of the most important 
unsolved problem in theoretical computer science is the P versus NP problem.
This question is one of the seven millennium prize problems.


%It asks if every problems can be solved and verified in polynomial time.
%Any NP problems can be reduced in polynomial time by a deterministic Turing machine to the SAT problems.
%The states that the propositional satisfiability problem is NP-complete
%SAT is NP because any assignment of Boolean values to Boolean variables that is claimed to 
%satisfy the given expression can be verified in polynomial time by a deterministic Turing machine. 
%
%


\subsection{Solving a SAT problem}

Two kinds of algorithm exists to solve satisfiability problems.
First, the \emph{incomplete} algorithm which does not provide any guarantee that will eventually report either any satisfiable assignment or declare that formula is unsatisfiable. This kind of algorithm is out of scope of this thesis. 
Second, the \emph{complete} algorithm, which provides a guarantee that if an assignment exists
it will be reached or it will declare that formula is unsatisfiable.
This section describes different \emph{complete }algorithm to solve a propositional formula.



\subsubsection{A naive algorithm}
A naive approach to solve a SAT problem is to try all possible assignments. In total,
for a propositional formula with $n$ variables, it leads to $2^n$ assignments in the worth case.  
\Cref{fig:naive_algo} illustrate the search tree for a given problem with six variables.
In this case $\alpha_{11}$ ($\neg x_1, \neg x_2, x_3, \neg x_4, x_5, \neg x_6 $) is found as a solution of the problem. In the general case,
this algorithm is obviously intractable on real problems even for a formula with few variables.


\begin{figure}[H]
	\centering
	\input{fig/naive_algorithm.tex}
	\caption{All possible assignments for a problem with 6 variables}
	\label{fig:naive_algo}
\end{figure}

%\hakan{Parler du DP}

\subsubsection{Davis Putnam Logemann Loveland (DPLL) algorithm}

One of the first non memory intensive algorithm to solve the SAT problems is 
the Davis Putnam Logemann Loveland (DPLL) algorithm~\cite{dpll_62}. 
It explores a binary tree using depth first search as given in \Cref{algo:dpll}.
The construction of the tree is related to a \emph{decision} literal (\cref{algo:dpll:decision}) then,
recursive call with each value are checked.
When a leaf report \unsat (\cref{algo:dpll:unsatbranch}), other branches are explored.
By recursive construction of the algorithm, when positive and negative value of a literal reach \unsat,
solver backtracks at most one level, this fact is called \emph{chronological backtracking}.
If all leaves report \unsat the formula $\varphi$ is unsatisfiable.
And so  if any branch found a solution  i.e. the problem is empty,
formula is satisfiable corresponding assignment is returned (\cref{algo:dpll:sat1,algo:dpll:sat2})

\input{algo/dpll}

An important function in the DPLL algorithm is \texttt{unitPropagation} \cref{algo:dpll:unit} and
it is detailed in \Cref{algo:unitdpll}. It searches all unit clauses to ensure satisfiability,
negative literals are removed from the clause that belongs to him. Then, given formula is either simplified
or leads to an inconsistency (empty clause). Unit literals are also added to the current assignment.

\input{algo/unitdpll}

When DPLL algorithm is executed on the formula in \Cref{fig:naive_algo}, unit propagation prevents to 
explore assignments from $\alpha_1 $ to $\alpha_{8}$. Moreover, application to unit propagation until fix point
leads directly to the solution. 
An important part of efficiency of DPLL is \texttt{assignDecisionLiteral} . The objective of this function 
is to find a literal that will generate a maximum of unit propagation. Intuitively, decision literals 
can be viewed as "guesses" and propagated literals can be viewed as "deductions". Finding a optimal variable
is NP-Hard \cite{biere2009handbook}. Different heuristics exists, one of them will be presented in 
\hakan{CITE SECTION DECISION HEURISTICS}.


%\input{algo/puredpll}
%Another idea introduced by DPLL was \emph{elimination of pure literals},
%a literal is said pure if it only appear on one sign (positive or negative) in the problem.
%These literals are set to true in the assignment.
%On consequence, all clauses that own these literals are satisfied and so can be removed.
%
\subsubsection{Conflict Driven Clause Learning (CDCL) algorithm}

The principal weakness of DPLL algorithm is to make same inconsistencies several times
(principally due to chronological backtracking), incurring unnecessary CPU usage.\\
Conflict Driven Clause Learning (CDCL) \cref{algo:cdcl} is another sound and complete algorithm
to resolve a SAT problem and overcome its principal weakness.


\Cref{algo:cdcl} gives an overview of CDCL, Like DPLL,  it walks a binary search tree.
It first applies unit propagation to the formula $\varphi$ for the current assignment $\alpha$ (\cref{alg:cdcl:unit}).
Note that it is exactly the same procedure as the one used for DPLL.
An inconsistency or a \emph{conflict} at level $0$ indicates that the formula is not satisfiable, and the algorithm
reports it (\cref{alg:cdcl:unsat_start,alg:cdcl:unsat_end}). When the conflict is occurring at a higher level, it
reason was analyzed and a clause called \emph{conflict clause} is deduced (\cref{alg:cdcl:analyze}).
 This clause is \emph{learnt} (\cref{alg:cdcl:learn}), as it does not change the
satisfiability of $\varphi$, and avoids encountering a conflict with the same
causes in the future. This function will be presented thereafter
The analysis is completed by the computation of a backjump point to which the algorithm backtracks (\cref{alg:cdcl:backjump}).
Finally, if no conflict appears, the algorithm chooses a new decision literal 
(\cref{alg:cdcl:pick_start,alg:cdcl:pick_end}).
The above steps are repeated until the satisfiability status of the
formula is determined.


\input{algo/cdcl}


%If a conflict is detected in higher level, it is analyzed, which provides a
%explaining the reason for the conflict.
%Different heuristics exists about the computation of conflict clause, on recent solvers
%the most used heuristic is the first Unique Implication Point ($1^{th}$ UIP) \cite{zhang2001efficient}.
%Restart is an important things in SAT solver, it allows solver to explore a new search space
%with the learned clauses. It is also finely intertwined with the decision heuristics.
%If the solver is working on "hard" part of the problem it will reconsider the decision variables and
%solve this part part at first. But if we restart too often the solver doesn't have to discover new things.
%
%
%  This clause is learnt (\cref{alg:cdcl:learn}), as it does not change the
%satisfiability of $\varphi$, and avoids encountering a conflict with the same
%causes in the future.
%
%
% The choice of the decision literal
%affect the performance of solver. The first most used heuristic is Variable State Independent Decaying Sum (VSIDS)~\cite{moskewicz2001chaff}. The idea behind this heuristic is that the "hard" parts of the search space 
%will be treated first. To do that, each variable has an activity and wa increase if it participate to the resolution
%of the conflict.
%The second most used heuristics is Learning rate based branching (LRB~\cite{liang2016learning})
%The above steps are repeated until the satisfiability status of the formula is determined.


\subsection{Conflict Analysis}
A conflict is an inconsistency discovered by the solver, a situation that requires for a variable to be set 
simultaneously to the \true and \false value. \Cref{fig:conflict} shows an assignments that leads to a conflict.
First the solver chose $\neg x_1$ as decision then $\neg x_6$ and then $\neg x_5$. This last one propagates $x_4$
which in turn propagates $x_2$ and $x_3$. On clause $\omega_1$, $x_3$ needs to be true and false in $\omega_5$ so
a conflict appears.
\begin{figure}[H]
	\centering
		\input{fig/conflict}
	\caption{Decisions/Propagations that leads to a  conflict}
	\label{fig:conflict}
\end{figure}

This series of decisions would provoke same propagation and leads to the same conflict. To avoid this
situation, the solver needs to analyze the reason of the conflict with so called \emph{implication graph}.
Implication graph represents the current state of the solver proof system. It records every dependencies
among variables and so updated when a variable is assigned either on decision or propagation and when a variable
is unassigned. The implication graph is a directed acyclic graph (DAG) in which a vertex represents an assigned variable labeled as $l@dl(l)$ where $l$ represents assigned literal and $dl(l)$ represents the decision level of the literal $l$.
\hakan{Parler de decision level et de trail dans l'algo cdcl au dessus}
Root vertexes , that have no incoming edges, are literals chosen by decision heuristics and others are 
propagated literals.
Incoming arcs labeled with a clause represents the \emph{reason} of this propagation.
This clause must be assertive i.e. all of its literals are false except one that are not yet assigned.
For example, \Cref{fig:implication-graph} shows implication graph of the previous example (\cref{fig:conflict}) until the conflict.

\begin{figure}[H]
	\centering
	\input{fig/implication_graph}
	\caption{Implication graph}
	\label{fig:implication-graph}
\end{figure}


\texttt{analyzeConflict} procedure analyzed this graph to found the reason of the conflict. To do that, a search of
\emph{unique implication point} (UIP) is performed. UIP of a decision level in the implication graph is a variables
which lies on every path from the decision to the conflict. Note that, there are many UIP for a given decision level.
In such case, UIPs are ordered according to the distance with the contradiction. The first UIP is the closest to
the conflict. It is well known that the first UIP provides the smallest set of assignment that is responsible for the
contradiction~\cite{zhang2001efficient}

An UIP divides the implication in two sides with a \emph{cut}, the reason side contains decision variables 
that is responsible for the contradiction and the conflict side contains the conflict. Note that, UIP is always is the 
reason side. \Cref{fig:implication-graph} depicts two cuts in the implication graph.

Once the reason side of a conflict is established, a conflict driven clause is produced called also 
\emph{learnt clause}. It purposes to avoid same contradiction. To build this clause, it suffice to negate 
literals that have an ongoing arc to the cut that contains first UIP. In \cref{fig:implication-graph}, produced
clause will be $\omega_l = \{x_1, \neg x_4 \}$. Since the information of this clause is redundant with respect to 
the original formula. It can be added without any satisfiability restrictions. 

After producing the learnt clause, the solver compute \texttt{backjumpAndRestartPolicies}. It will unassigned all
decision until the first one that is responsible for the conflict and so prune search space that contains no solution.
This is the key point of the CDCL algorithm. The restart policy will be discussed in the next section.
In our example, the target decision level is one. A first UIP variable  is said assertive, and will be propagated in the next loop of the solving algorithm.
If a conflict implied only one level, the decision variable must 
be assigned to the opposite value at level zero. Roughly speaking, to ensure satisfiability of the formula, this 
literal must be true without any decision.

 

\subsection{Heuristics}
This sections gives an overview of different heuristics present in modern SAT solvers.
In particular 

\textbf{Decision heuristics}. Variable used to divide problems have a huge impact on the 
overall solving time by the solver. Decision variable may impact the number of propagation and so the depth of the search tree  As propagated one can be seen as \emph{deduction} and the 


\textbf{Restart}


\hakan{Peut Ãªtre mettre les heuristiques dans des paragraphes}

\hakan{Literal Block Distance LBD}

\section{Groups basics}

Symmetries is related to a branch of mathematics called group theory. This section give us an overview of group
theory.

\subsection{Groups}

A \emph{group} is a structure $\langle G, * \rangle$, where $G$ is a non empty set and $*$ a binary
operation such the following axioms are satisfied:
\begin{itemize}[noitemsep,nolistsep]
	\item \emph{associativity}: $\forall a, b, c \in G, (a * b) * c = a * (b * c)$
	\item \emph{closure}: $\forall a, b \in G, a * b \in G$.
	\item \emph{identity}: $\forall a \in G, \exists e$ such that $ a * e = e * a = a$
	\item \emph{inverse}:  $\forall a \in G, \exists b \in G$, commonly denoted $a^{-1}$ such that
	 $a * a^{-1} = a^{-1} * a = e$
\end{itemize}

Note that \emph{commutativity} is not required i.e $\ a * b = b * a$, for $a, b \in G$.
The group is \emph{abelian} if it satisfies the commutativity rule.
Moreover, the last definition leads to important properties which are: i) uniqueness of the identity element. 
To prove this property, assume $\langle G, * \rangle$ a group with two identity elements $e$ and $f$ 
then $ e = e * f = f$.
ii) uniqueness of the inverse element. To prove this property, suppose that an element $x_1$ has two inverses,
denoted $b$ and $c$ in group $\langle G, * \rangle$, then\\
	$\begin{array}{lcll}					
			b & = & b * e & \\
			  & = & b * (a * c) & c \text{ is an inverse of } a, \text{so } e = a * c\\
			  & = & (b * a) * c &   \text{\emph{associativity} rule}\\
			  & = & e * c       & b \text{ is an inverse of } a, \text{so } e = a * b\\
			  & = & c           &   \text{\emph{identity} rule}
	\end{array}$

The structure $\langle G, * \rangle$ is denoted as G when clear from context that G is a group
with a binary operation. In this thesis, we interested only with the \emph{finite} groups i.e
with a finite number of elements.

Given a group $G$, a \emph{subgroup} is a non empty subset of $G$ which is also a group with 
the same binary operation. If $H$ is a subgroup of $G$, we denote as $H \leq G$.
A group has at least two subgroups: i) the subgroup composed by identity element $\{e\}$, denoted \emph{trivial} subgroup. All other subgroups are \emph{nontrivial}; ii) the subgroup composed by itself, denoted \emph{improper} subgroup. All other subgroups are \emph{proper}.


\subsubsection{Generators of a group}

If every elements in a group G can be expressed as a linear combination
of a set of group of elements S = $\{g_1, g_2, ..., g_n \}$ then we say G is 
generated by the S. we denote this as G = $\langle S \rangle$ =
$\langle \{g_1, g_2, ..., g_n \} \rangle$ 



\subsection{Permutation groups}
 
A \emph{permutation} is a bijection from a set $X$ to itself.\\
 Example: given a set $X = \{x_1, x_2, x_3, x_4, x_5, x_6\}$,
$g = ${\Bigg( \begin{tabular}{cccccc}
		$x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ & $x_6$\\
		$x_2$ & $x_3$ & $x_1$ & $x_4$ & $x_6$ & $x_5$
	\end{tabular} \Bigg)}\\
$g$ is a permutation that maps $x_1$ to $x_2$, $x_2$ to $x_3$, $x_3$ to $x_1$, $x_4$ to $x_4$, $x_5$ to $x_6$ and $x_6$ to $x_5$.

Permutations are generally written in \emph{cycle notation} and the self mapped elements are omitted.
So the permutation in cycle notation will be : $g$ = ($x_1$ $x_2$ $x_3$) ($x_5$ $x_6$).
We say \emph{support} of the permutation $g$ noted $supp(g)$ the elements that not mapped to themselves,
$supp(g) = \{ x \in X \mid g.x \neq x\}$. A variable $x$ is \emph{stabilized} by a permutation $g$ 
if $x \notin \support(g)$. A clause $\omega$ is \emph{stabilized} by a permutation $g$ if 
$\omega \cap \support(g) = \emptyset$. \hakan{Maybe stabilisation as definition ?}


The set of permutations of a given set $X$ form a group $G$,
with the composition operation ($\circ$) and called \emph{permutation group}.
The \emph{symmetric group} id the set of all possible permutations of a set $X$ and noted \Group($X$).
%The set of \textbf{all} permutations of a set $X$ is the \emph{symmetric group} of $X$ and noted \Group($X$).
So, a \emph{permutation group} is a subgroup of \Group($X$). 
%A set of permutations $P$ is a set of \emph{generators} of a group $G$ if each permutation of $G$
%can be expressed as a composition of permutations in $P$. 


A permutation group $G$ induces a \emph{equivalence relation} on the set of element $X$ being
permuted. Two elements $x_1, x_2 \in X$ are equivalent if there exists a permutation $g \in G$ such that
$g x_1 = x_2$. Then equivalence relation partitions $X$ into \emph{equivalence classes} referred to
as the \emph{orbits} of $X$ under $G$. The orbit of an element $x$ under group $G$ (or simply orbit of $x$ when clear
from the context) is the set $[x]_G = \{g.x \mid g \in G\}$




