\chapter{Preliminaries}\label{chap:preliminaries}

\section{SAT basics}

\subsection{Satisfiability problem}
A \emph{Boolean variable}, or \emph{propositional variable}, is a variable that
has two possible values : true or false (noted respectively $\true$ or $\false$).
A \emph{literal} $l$ is a propositional variable or its
negation. For a given variable $x$, the positive literal is represented by $x$
and the negative one by $\neg x$.
A \emph{clause} $\omega$ is a finite disjunction of literals represented
equivalently by $\omega = \bigvee_{i=1}^k l_i$ or the set of its literals
$\omega = \{l_i\}_{i \in \llbracket 1,k \rrbracket}$. A clause with a single
literal is called \emph{unit clause}.
A \emph{conjunctive normal form (CNF) formula} $\varphi$ is a finite
conjunction of clauses.  A CNF can be either noted $\varphi = \bigwedge_{i=1}^k
\omega_i$ or $\varphi = \{\omega_i\}_{i \in \llbracket 1,k \rrbracket}$. We
denote $\Vars_\varphi$ ($\Lits_\varphi$) the set of variables (literals) used in
$\varphi$ (the index in $\Vars_\varphi$ and $\Lits_\varphi$ is usually omitted when
clear from context).

For a given formula $\varphi$, an \emph{assignment} of the variables of
$\varphi$ is a function $\alpha: \Vars \mapsto \{ \true, \false \}$.  As usual, $\alpha$ is
\emph{total}, or \emph{complete}, when all elements of $\Vars$ have an image by
$\alpha$, otherwise it is \emph{partial}. By abuse of notation, an assignment is
often represented by the set of its true literals.  The set of all (possibly
partial) assignments of $\Vars$ is noted $\Assignments(\Vars)$.

The assignment $\alpha$ \emph{satisfies} the clause $\omega$, denoted $\alpha
\models \omega$, if $\alpha \cap \omega \neq \emptyset$. Similarly, the assignment
$\alpha$ satisfies the propositional formula $\varphi$, denoted $\alpha \models
\varphi$, if $\alpha$ satisfies all the clauses of $\varphi$. Note that a
formula may be satisfied by a partial assignment. In this case, unassigned variable are called
\emph{dont care}.
A formula is said to be
\emph{satisfiable} (\sat) if there is at least one assignment that satisfies it;
otherwise the formula is \emph{unsatisfiable} (\unsat).

\subsection{An NP-complete problem}

The SAT problems is the first NP-complete algorithm as proven by Stephen Cook in 1971~\cite{cook1971complexity}.
It is also proved by Leonid Levin in 1973~\cite{4640789}, for this purpose, it was known as Cook-Levin theorem.
The proof of to show that can be found in \cite{sipser2006introduction}.
Any NP problem can be reduced to a SAT problem in polynomial time and so open one of the most important 
unsolved problem in theoretical computer science is the P versus NP problem.
This question is one of the seven millennium prize problems.


%It asks if every problems can be solved and verified in polynomial time.
%Any NP problems can be reduced in polynomial time by a deterministic Turing machine to the SAT problems.
%The states that the propositional satisfiability problem is NP-complete
%SAT is NP because any assignment of Boolean values to Boolean variables that is claimed to 
%satisfy the given expression can be verified in polynomial time by a deterministic Turing machine. 
%
%


\subsection{Solving a SAT problem}

Two kinds of algorithm exists to solve satisfiability problems.
First, the \emph{incomplete} algorithm which does not provide any guarantee that will eventually report either any satisfiable assignment or declare that formula is unsatisfiable. This kind of algorithm is out of scope of this thesis. 
Second, the \emph{complete} algorithm, which provides a guarantee that if an assignment exists
it will be reached or it will declare that formula is unsatisfiable.
This section describes different \emph{complete }algorithm to solve a propositional formula.



\subsubsection{A naive algorithm}
A naive approach to solve a SAT problem is to try all possible assignments. In total,
for a propositional formula with $n$ variables, it leads to $2^n$ assignments in the worth case.  
\Cref{fig:naive_algo} illustrate the search tree for a given problem with six variables.
In this case $\alpha_{11}$ ($\neg x_1, \neg x_2, x_3, \neg x_4, x_5, \neg x_6 $) is found as a solution of the problem. In the general case,
this algorithm is obviously intractable on real problems even for a formula with few variables.


\begin{figure}[H]
	\centering
	\input{fig/naive_algorithm.tex}
	\caption{All possible assignments for a problem with 6 variables}
	\label{fig:naive_algo}
\end{figure}


\subsubsection{Davis Putnam Logemann Loveland (DPLL) algorithm}

One of the first non memory intensive algorithm to solve the SAT problems is 
the Davis Putnam Logemann Loveland (DPLL) algorithm~\cite{dpll_62}. 
It explores a binary tree using depth first search as given in \Cref{algo:dpll}.
The construction of the tree is related to a \emph{decision} literal (\cref{algo:dpll:decision}) then,
recursive call with each value are checked.
When a leaf report \unsat (\cref{algo:dpll:unsatbranch}), other branches are explored.
By recursive construction of the algorithm, when positive and negative value of a literal reach \unsat,
solver backtracks at most one level, this fact is called \emph{chronological backtracking}.
If all leaves report \unsat the formula $\varphi$ is unsatisfiable.
And so  if any branch found a solution  i.e. the problem is empty,
formula is satisfiable corresponding assignment is returned (\cref{algo:dpll:sat1,algo:dpll:sat2})

\input{algo/dpll}

An important function in the DPLL algorithm is \texttt{unitPropagation} \cref{algo:dpll:unit} and
it is detailed in \Cref{algo:unitdpll}. It searches all unit clauses to ensure satisfiability,
negative literals are removed from the clause that belongs to him. Then, given formula is either simplified
or leads to an inconsistency (empty clause). Unit literals are also added to the current assignment.

\input{algo/unitdpll}

When DPLL algorithm is executed on the formula in \Cref{fig:naive_algo}, unit propagation prevents to 
explore assignments from $\alpha_1 $ to $\alpha_{8}$. Moreover, application to unit propagation until fix point
leads directly to the solution. 
An important part of efficiency of DPLL is \texttt{assignDecisionLiteral} . The objective of this function 
is to find a literal that will generate a maximum of unit propagation. Intuitively, decision literals 
can be viewed as "guesses" and propagated literals can be viewed as "deductions". Finding a optimal variable
is NP-Hard \cite{biere2009handbook}. Different heuristics exists, one of them will be presented in \ref{sec:heuristics}.


%\input{algo/puredpll}
%Another idea introduced by DPLL was \emph{elimination of pure literals},
%a literal is said pure if it only appear on one sign (positive or negative) in the problem.
%These literals are set to true in the assignment.
%On consequence, all clauses that own these literals are satisfied and so can be removed.
%
\subsubsection{Conflict Driven Clause Learning (CDCL) algorithm}

The principal weakness of DPLL algorithm is to make same inconsistencies several times
(principally due to chronological backtracking), incurring unnecessary CPU usage.\\
Conflict Driven Clause Learning (CDCL) \cref{algo:cdcl} is another sound and complete algorithm
to resolve a SAT problem and overcome its principal weakness.


\Cref{algo:cdcl} gives an overview of CDCL, Like DPLL,  it walks a binary search tree.
It first applies unit propagation to the formula $\varphi$ for the current assignment $\alpha$ (\cref{alg:cdcl:unit}).
Note that it is exactly the same procedure as the one used for DPLL.
An inconsistency or a \emph{conflict} at level $0$ indicates that the formula is not satisfiable, and the algorithm
reports it (\cref{alg:cdcl:unsat_start,alg:cdcl:unsat_end}). When the conflict is occurring at a higher level, it
reason was analyzed and a clause called \emph{conflict clause} is deduced (\cref{alg:cdcl:analyze}).
 This clause is \emph{learnt} (\cref{alg:cdcl:learn}), as it does not change the
satisfiability of $\varphi$, and avoids encountering a conflict with the same
causes in the future. This function will be presented thereafter
The analysis is completed by the computation of a backjump point to which the algorithm backtracks (\cref{alg:cdcl:backjump}).
Finally, if no conflict appears, the algorithm chooses a new decision literal 
(\cref{alg:cdcl:pick_start,alg:cdcl:pick_end}).
The above steps are repeated until the satisfiability status of the
formula is determined.


\input{algo/cdcl}

\subsection{Conflict Analysis}
A conflict is an inconsistency discovered by the solver, a situation that requires for a variable to be set 
simultaneously to the \true and \false value. \Cref{fig:conflict} shows an assignments that leads to a conflict.
First the solver chose $\neg x_1$ as decision then $\neg x_6$ and then $\neg x_5$. This last one propagates $x_4$
which in turn propagates $x_2$ and $x_3$. On clause $\omega_1$, $x_3$ needs to be true and false in $\omega_5$ so
a conflict appears.
\begin{figure}[H]
	\centering
		\input{fig/conflict}
	\caption{Decisions/Propagations that leads to a  conflict}
	\label{fig:conflict}
\end{figure}

This series of decisions would provoke same propagation and leads to the same conflict. To avoid this
situation, the solver needs to analyze the reason of the conflict with so called \emph{implication graph}.
Implication graph represents the current state of the solver proof system. It records every dependencies
among variables and so updated when a variable is assigned either on decision or propagation and when a variable
is unassigned. The implication graph is a directed acyclic graph (DAG) in which a vertex represents an assigned variable labeled as $l@dl(l)$ where $l$ represents assigned literal and $dl(l)$ represents the decision level of the literal $l$.
\hakan{Parler de decision level et de trail dans l'algo cdcl au dessus}
Root vertexes , that have no incoming edges, are literals chosen by decision heuristics and others are 
propagated literals.
Incoming arcs labeled with a clause represents the \emph{reason} of this propagation.
This clause must be assertive i.e. all of its literals are false except one that are not yet assigned.
For example, \Cref{fig:implication-graph} shows implication graph of the previous example (\cref{fig:conflict}) until the conflict.

\begin{figure}[H]
	\centering
	\input{fig/implication_graph}
	\caption{Implication graph}
	\label{fig:implication-graph}
\end{figure}


\texttt{analyzeConflict} procedure analyzed this graph to found the reason of the conflict. To do that, a search of
\emph{unique implication point} (UIP) is performed. UIP of a decision level in the implication graph is a variables
which lies on every path from the decision to the conflict. Note that, there are many UIP for a given decision level.
In such case, UIPs are ordered according to the distance with the contradiction. The first UIP is the closest to
the conflict. It is well known that the first UIP provides the smallest set of assignment that is responsible for the
contradiction~\cite{zhang2001efficient}

An UIP divides the implication in two sides with a \emph{cut}, the reason side contains decision variables 
that is responsible for the contradiction and the conflict side contains the conflict. Note that, UIP is always is the 
reason side. \Cref{fig:implication-graph} depicts two cuts in the implication graph.

Once the reason side of a conflict is established, a conflict driven clause is produced called also 
\emph{learnt clause}. It purposes to avoid same contradiction. To build this clause, it suffice to negate 
literals that have an ongoing arc to the cut that contains first UIP. In \cref{fig:implication-graph}, produced
clause will be $\omega_l = \{x_1, \neg x_4 \}$. Since the information of this clause is redundant with respect to 
the original formula. It can be added without any satisfiability restrictions. 

After producing the learn clause, the solver compute \texttt{backjumpAndRestartPolicies}. It will unassigned all
decision until the first one that is responsible for the conflict and so prune search space that contains no solution.
This is the key point of the CDCL algorithm. The restart policy will be discussed in the next section.
In our example, the target decision level is one. A first UIP variable  is said assertive, and will be propagated in the next loop of the solving algorithm.
If a conflict implied only one level, the decision variable must 
be assigned to the opposite value at level zero. Roughly speaking, to ensure satisfiability of the formula, this 
literal must be true without any decision.

 
\subsection{Heuristics}\label{sec:heuristics}
This sections gives an overview of different heuristics present in modern SAT solvers.

\textbf{Decision heuristics}. Variable used to divide problems have a huge impact on the 
overall solving time by the solver. Decision variable may impact the number of propagation and so 
the depth of the search tree.

Variable State Independent Decaying Sum (VSIDS)~\cite{moskewicz2001chaff} is one of the decision heuristic and used
nowadays in almost all solvers. Each variable has an activity and was increased by a multiplicative factor 
when it participate to the resolution of the conflict.
A solver has thousands conflicts during the solving and so activity of variables are very volatile.
Decision heuristics choose unassigned variable with the highest activity.
The idea behind this heuristics is to solve "hard" part of problem at the top of the search tree.
Hence, it is much more efficient when coupled with the restart heuristics. 

Learning rate based branching (LRB~\cite{liang2016learning}) is a most recent decision heuristics. It is a
generalization of VSIDS and its goal is to optimize the \emph{learning rate} (LR), defined as the ability to generate
learnt clauses. The LRB of a variable is the weighted average (computed with \emph{exponential recency
weighted average} (ERWA))  value taken by its LR over the time. Unassigned variable with a highest LRB are chose as decision. The idea behind this heuristics is to keep variables that used to generate learnt clause in the search tree.


\textbf{Restarts.}
Another important heuristic is \emph{restart}. Basically the solver abandons it current assignment and so 
start from the top of the tree, while maintaining other information notably learnt clauses but also scores of variables in the decision heuristic. It prevents the solver to get stuck in "hard" (heavy tailing~\cite{gomes1997heavy} part of the search space and can not escape due to backjump few levels after conflict resolution. Restart is best effort heuristics, hoping that,
with more information, a better assignments was made. Hence, in practice, SAT solvers usually restarts after a
certain number of conflict. Empirically a solver with restart has a better results~\cite{huang2007effect} and is today
used in almost all state of the art solvers.


\subsection{Preprocessing / Inprocessing}

In order to optimize resolution time by the solver, some transformation to simplify the original formula can be applied. This is done by \emph{preprocessing} engine before the start of solving. When it is used at some point during the solving, usually after a restart, it is called \emph{inprocessing}.

Simplification of the formula is made by removing clauses and/or variables.\\
\textbf{Variable elimination} simplification is based on \emph{Resolution inference rule}.
Suppose two clauses	$\omega_1 = \{x_1, x_i, ..., x_j \}$ and $\omega_2 = \{\neg x_1, y_i, ..., y_j\}$.
The resolution inference rule allows to derive clause $\omega_3 = \{x_i, ..., x_j, y_i, ..., y_j\}$ which is called
the \emph{resolvent} as it results from resolving two clauses on the literal $x_1$ and $\neg x_1$.
Moreover, this rule is a complete algorithm to solve a SAT problem. Applying variable elimination until either an empty clause is derived (unsatisfiable formula) or no more application of the resolution are
possible (satisfiable formula). Its major issue is to explicitly generate all resolvent can be exponential in CNF size. Hence, the memory of computer will be limiting factor.


\emph{Subsumption} is a simple principle to remove clauses. Suppose two clauses $\omega_1$ and $\omega_2$ such that
$\omega_1 \subset  \omega_2$, then $\omega_2$ can be safely removed from the original formula.
\emph{Self Subsuming resolution} is a principle that use resolution rule and subsumption.
Resolvent clause subsumes the original one. Example $\omega_1 = \{x_1, \neg x_2, x_3\}$ and $\omega_2 = \{x_1, \neg x_2, x_3, x_4\}$, then resolvent clause will be $\omega_3 = \{x_1, x_3\}$ which subsumes $\omega_2$. This principle
is presents in \texttt{SatElite}~\cite{een2005effective} preprocessor engine and used in almost all modern SAT solvers.

Other simplification techniques exists such that \emph{Gaussian elimination} which detect sub formula in a xor-SAT
form and solve it in a polynomial complexity. Moreover, this technique can also be used as inprocessing~\cite{soos2010enhanced}. 

Some techniques exploits the structure of the original formula and add relevant clauses to speed up the resolution
time of the SAT solver. One of them use community structure of the formula to find good clauses to add into.
A preprocessor engine doing that is  \texttt{modprep}~\cite{ansotegui2015using}.
Usage of symmetries also add relevant clause in the formula and will be detailed in the next chapter.


\subsection{Parallel SAT solving}
With the emergence of multi core architectures and increasing power of computer, one way to optimize the solving
of a SAT problem is the exploitation of these cores. Effectively, SAT problems are a good candidate for parallelism.
\emph{Portfolio} is a technique that launches several SAT solver in parallel with different heuristics (decisions, restarts, ...) that communicates or not between us. When one of them found a solution or found that none exists, the overall computation is finished. Another technique to make parallel SAT solver exists and called \emph{divide and conquer} in which the search space was divided dynamically according to positive and negative value of the decision
literal. Several solvers cooperate to found solution, each of them are assigned to sub formula induced by the division. Some specific techniques like load balancing and work stealing is applied to avoid a solver to be idle.
A recent framework \emph{PaInleSS} (a Framework for Parallel SAT Solving) can be used to easily create a new parallel 
SAT solver with different heuristics~\cite{le2017painless}~\cite{le2019modular}. Authors of this framework win the parallel tracks of SAT competition \footnote{\url{http://www.satcompetition.org/}} in 2018.






\section{Groups basics}

Some CNF instances presents symmetries and use of this property allow the solver to cut off search space.
Symmetries is related to a branch of mathematics called group theory. This section give us an overview of group
theory.

\subsection{Groups}

A \emph{group} is a structure $\langle G, * \rangle$, where $G$ is a non empty set and $*$ a binary
operation such the following axioms are satisfied:
\begin{itemize}[noitemsep,nolistsep]
	\item \emph{associativity}: $\forall a, b, c \in G, (a * b) * c = a * (b * c)$
	\item \emph{closure}: $\forall a, b \in G, a * b \in G$.
	\item \emph{identity}: $\forall a \in G, \exists e$ such that $ a * e = e * a = a$
	\item \emph{inverse}:  $\forall a \in G, \exists b \in G$, commonly denoted $a^{-1}$ such that
	 $a * a^{-1} = a^{-1} * a = e$
\end{itemize}

Note that \emph{commutativity} is not required i.e $\ a * b = b * a$, for $a, b \in G$.
The group is \emph{abelian} if it satisfies the commutativity rule.
Moreover, the last definition leads to important properties which are: i) uniqueness of the identity element. 
To prove this property, assume $\langle G, * \rangle$ a group with two identity elements $e$ and $f$ 
then $ e = e * f = f$.
ii) uniqueness of the inverse element. To prove this property, suppose that an element $x_1$ has two inverses,
denoted $b$ and $c$ in group $\langle G, * \rangle$, then\\
	$\begin{array}{lcll}					
			b & = & b * e & \\
			  & = & b * (a * c) & c \text{ is an inverse of } a, \text{so } e = a * c\\
			  & = & (b * a) * c &   \text{\emph{associativity} rule}\\
			  & = & e * c       & b \text{ is an inverse of } a, \text{so } e = a * b\\
			  & = & c           &   \text{\emph{identity} rule}
	\end{array}$

The structure $\langle G, * \rangle$ is denoted as G when clear from context that G is a group
with a binary operation. In this thesis, we interested only with the \emph{finite} groups i.e
with a finite number of elements.

Given a group $G$, a \emph{subgroup} is a non empty subset of $G$ which is also a group with 
the same binary operation. If $H$ is a subgroup of $G$, we denote as $H \leq G$.
A group has at least two subgroups: i) the subgroup composed by identity element $\{e\}$, denoted \emph{trivial} subgroup. All other subgroups are \emph{nontrivial}; ii) the subgroup composed by itself, denoted \emph{improper} subgroup. All other subgroups are \emph{proper}.


\subsubsection{Generators of a group}

If every elements in a group G can be expressed as a linear combination
of a set of group of elements S = $\{g_1, g_2, ..., g_n \}$ then we say G is 
generated by the S. we denote this as G = $\langle S \rangle$ =
$\langle \{g_1, g_2, ..., g_n \} \rangle$ 



\subsection{Permutation groups}
 
A \emph{permutation} is a bijection from a set $X$ to itself.\\
 Example: given a set $X = \{x_1, x_2, x_3, x_4, x_5, x_6\}$,
$g = ${\Bigg( \begin{tabular}{cccccc}
		$x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ & $x_6$\\
		$x_2$ & $x_3$ & $x_1$ & $x_4$ & $x_6$ & $x_5$
	\end{tabular} \Bigg)}\\
$g$ is a permutation that maps $x_1$ to $x_2$, $x_2$ to $x_3$, $x_3$ to $x_1$, $x_4$ to $x_4$, $x_5$ to $x_6$ and $x_6$ to $x_5$.

Permutations are generally written in \emph{cycle notation} and the self mapped elements are omitted.
So the permutation in cycle notation will be : $g$ = ($x_1$ $x_2$ $x_3$) ($x_5$ $x_6$).
We say \emph{support} of the permutation $g$ noted $supp(g)$ the elements that not mapped to themselves,
$supp(g) = \{ x \in X \mid g.x \neq x\}$. A variable $x$ is \emph{stabilized} by a permutation $g$ 
if $x \notin \support(g)$. A clause $\omega$ is \emph{stabilized} by a permutation $g$ if 
$\omega \cap \support(g) = \emptyset$. \hakan{Maybe stabilisation as definition ?}


The set of permutations of a given set $X$ form a group $G$,
with the composition operation ($\circ$) and called \emph{permutation group}.
The \emph{symmetric group} id the set of all possible permutations of a set $X$ and noted \Group($X$).
%The set of \textbf{all} permutations of a set $X$ is the \emph{symmetric group} of $X$ and noted \Group($X$).
So, a \emph{permutation group} is a subgroup of \Group($X$). 
%A set of permutations $P$ is a set of \emph{generators} of a group $G$ if each permutation of $G$
%can be expressed as a composition of permutations in $P$. 


A permutation group $G$ induces a \emph{equivalence relation} on the set of element $X$ being
permuted. Two elements $x_1, x_2 \in X$ are equivalent if there exists a permutation $g \in G$ such that
$g x_1 = x_2$. Then equivalence relation partitions $X$ into \emph{equivalence classes} referred to
as the \emph{orbits} of $X$ under $G$. The orbit of an element $x$ under group $G$ (or simply orbit of $x$ when clear
from the context) is the set $[x]_G = \{g.x \mid g \in G\}$




